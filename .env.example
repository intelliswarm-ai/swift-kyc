# LLM Configuration
# Use Ollama for local, confidential processing (recommended)
USE_OLLAMA=true

# Ollama Configuration (when USE_OLLAMA=true)
OLLAMA_MODEL=mistral  # Options: llama2, mistral, mixtral, neural-chat, etc.
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI Configuration (when USE_OLLAMA=false)
# WARNING: Using OpenAI will send client data to external servers
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL_NAME=gpt-4

# Headless Browser Configuration
BROWSER_HEADLESS=true
BROWSER_TIMEOUT=30000

# Database Configuration
PEP_DATABASE_PATH=./data/pep_database.json
PEP_DATA_DIR=./data
PEP_SOURCES_CONFIG=./config/pep_sources.json
SANCTIONS_LIST_PATH=./data/sanctions_lists.json

# PEP Search Configuration
ENABLE_ONLINE_PEP_SEARCH=true
PEP_CACHE_DURATION_HOURS=24

# Risk Assessment Thresholds
HIGH_RISK_THRESHOLD=0.7
MEDIUM_RISK_THRESHOLD=0.4

# Report Output Directory
REPORT_OUTPUT_DIR=./reports

# Proxy Configuration (optional)
# HTTP_PROXY=http://proxy.example.com:8080
# HTTPS_PROXY=https://proxy.example.com:8080