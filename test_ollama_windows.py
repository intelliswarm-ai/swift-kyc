#!/usr/bin/env python3
"""
Test Ollama on Windows
"""
import requests
import json
import os

print("ğŸ” Testing Ollama on Windows")
print("=" * 60)

# Test connection
url = "http://localhost:11434"
print(f"\nğŸ“¡ Testing connection to: {url}")

try:
    # Check version
    response = requests.get(f"{url}/api/version", timeout=5)
    if response.status_code == 200:
        print(f"âœ… Ollama is running!")
        print(f"   Version: {response.json()}")
    else:
        print(f"âŒ Ollama returned status code: {response.status_code}")
        
    # Check models
    print("\nğŸ“‹ Checking available models...")
    response = requests.get(f"{url}/api/tags", timeout=5)
    if response.status_code == 200:
        models = response.json().get('models', [])
        if models:
            print("âœ… Available models:")
            for model in models:
                print(f"   - {model['name']} ({model.get('size', 'unknown size')})")
        else:
            print("âš ï¸  No models installed")
            print("\nğŸ“ To install a model:")
            print("   ollama pull mistral")
            print("   ollama pull llama2")
    
    # Test generation
    print("\nğŸ§ª Testing text generation...")
    data = {
        "model": "mistral",
        "prompt": "Say exactly: 'Hello from Ollama'",
        "stream": False
    }
    
    response = requests.post(f"{url}/api/generate", json=data, timeout=30)
    if response.status_code == 200:
        result = response.json()
        print(f"âœ… Generation successful!")
        print(f"   Response: {result.get('response', '').strip()}")
    else:
        print(f"âŒ Generation failed: {response.status_code}")
        
except requests.exceptions.ConnectionError:
    print("\nâŒ Cannot connect to Ollama!")
    print("\nğŸ“ To start Ollama:")
    print("  1. Open a new terminal")
    print("  2. Run: ollama serve")
    print("\nâš ï¸  Make sure Ollama is installed:")
    print("  Download from: https://ollama.ai/download")
    
except Exception as e:
    print(f"\nâŒ Error: {type(e).__name__}: {str(e)}")

# Test with langchain-ollama
print("\n\nğŸ§ª Testing langchain-ollama integration...")
try:
    from langchain_ollama import OllamaLLM
    print("âœ… langchain-ollama imported successfully")
    
    # Create LLM instance
    llm = OllamaLLM(model="mistral")
    print("âœ… OllamaLLM instance created")
    
    # Test invoke
    response = llm.invoke("Say 'Integration test successful'")
    print(f"âœ… LLM response: {response}")
    
except ImportError:
    print("âŒ langchain-ollama not installed")
    print("   Run: pip install langchain-ollama")
except Exception as e:
    print(f"âŒ LLM test failed: {type(e).__name__}: {str(e)}")

print("\n" + "=" * 60)
print("Test complete!")